{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim import Adamax\n",
    "import wandb\n",
    "import yaml\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from set_seed import set_seed\n",
    "from transformers import AutoTokenizer\n",
    "import torchmetrics\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from models import SBERT_base_Model, BERT_base_Model\n",
    "from datasets import KorSTSDatasets, Collate_fn, bucket_pair_indices, KorSTSDatasets_for_BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bert.embeddings.word_embeddings.weight\n",
      "1 bert.embeddings.position_embeddings.weight\n",
      "2 bert.embeddings.token_type_embeddings.weight\n",
      "3 bert.embeddings.LayerNorm.weight\n",
      "4 bert.embeddings.LayerNorm.bias\n",
      "5 bert.encoder.layer.0.attention.self.query.weight\n",
      "6 bert.encoder.layer.0.attention.self.query.bias\n",
      "7 bert.encoder.layer.0.attention.self.key.weight\n",
      "8 bert.encoder.layer.0.attention.self.key.bias\n",
      "9 bert.encoder.layer.0.attention.self.value.weight\n",
      "10 bert.encoder.layer.0.attention.self.value.bias\n",
      "11 bert.encoder.layer.0.attention.output.dense.weight\n",
      "12 bert.encoder.layer.0.attention.output.dense.bias\n",
      "13 bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "14 bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "15 bert.encoder.layer.0.intermediate.dense.weight\n",
      "16 bert.encoder.layer.0.intermediate.dense.bias\n",
      "17 bert.encoder.layer.0.output.dense.weight\n",
      "18 bert.encoder.layer.0.output.dense.bias\n",
      "19 bert.encoder.layer.0.output.LayerNorm.weight\n",
      "20 bert.encoder.layer.0.output.LayerNorm.bias\n",
      "21 bert.encoder.layer.1.attention.self.query.weight\n",
      "22 bert.encoder.layer.1.attention.self.query.bias\n",
      "23 bert.encoder.layer.1.attention.self.key.weight\n",
      "24 bert.encoder.layer.1.attention.self.key.bias\n",
      "25 bert.encoder.layer.1.attention.self.value.weight\n",
      "26 bert.encoder.layer.1.attention.self.value.bias\n",
      "27 bert.encoder.layer.1.attention.output.dense.weight\n",
      "28 bert.encoder.layer.1.attention.output.dense.bias\n",
      "29 bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "30 bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "31 bert.encoder.layer.1.intermediate.dense.weight\n",
      "32 bert.encoder.layer.1.intermediate.dense.bias\n",
      "33 bert.encoder.layer.1.output.dense.weight\n",
      "34 bert.encoder.layer.1.output.dense.bias\n",
      "35 bert.encoder.layer.1.output.LayerNorm.weight\n",
      "36 bert.encoder.layer.1.output.LayerNorm.bias\n",
      "37 bert.encoder.layer.2.attention.self.query.weight\n",
      "38 bert.encoder.layer.2.attention.self.query.bias\n",
      "39 bert.encoder.layer.2.attention.self.key.weight\n",
      "40 bert.encoder.layer.2.attention.self.key.bias\n",
      "41 bert.encoder.layer.2.attention.self.value.weight\n",
      "42 bert.encoder.layer.2.attention.self.value.bias\n",
      "43 bert.encoder.layer.2.attention.output.dense.weight\n",
      "44 bert.encoder.layer.2.attention.output.dense.bias\n",
      "45 bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "46 bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "47 bert.encoder.layer.2.intermediate.dense.weight\n",
      "48 bert.encoder.layer.2.intermediate.dense.bias\n",
      "49 bert.encoder.layer.2.output.dense.weight\n",
      "50 bert.encoder.layer.2.output.dense.bias\n",
      "51 bert.encoder.layer.2.output.LayerNorm.weight\n",
      "52 bert.encoder.layer.2.output.LayerNorm.bias\n",
      "53 bert.encoder.layer.3.attention.self.query.weight\n",
      "54 bert.encoder.layer.3.attention.self.query.bias\n",
      "55 bert.encoder.layer.3.attention.self.key.weight\n",
      "56 bert.encoder.layer.3.attention.self.key.bias\n",
      "57 bert.encoder.layer.3.attention.self.value.weight\n",
      "58 bert.encoder.layer.3.attention.self.value.bias\n",
      "59 bert.encoder.layer.3.attention.output.dense.weight\n",
      "60 bert.encoder.layer.3.attention.output.dense.bias\n",
      "61 bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "62 bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "63 bert.encoder.layer.3.intermediate.dense.weight\n",
      "64 bert.encoder.layer.3.intermediate.dense.bias\n",
      "65 bert.encoder.layer.3.output.dense.weight\n",
      "66 bert.encoder.layer.3.output.dense.bias\n",
      "67 bert.encoder.layer.3.output.LayerNorm.weight\n",
      "68 bert.encoder.layer.3.output.LayerNorm.bias\n",
      "69 bert.encoder.layer.4.attention.self.query.weight\n",
      "70 bert.encoder.layer.4.attention.self.query.bias\n",
      "71 bert.encoder.layer.4.attention.self.key.weight\n",
      "72 bert.encoder.layer.4.attention.self.key.bias\n",
      "73 bert.encoder.layer.4.attention.self.value.weight\n",
      "74 bert.encoder.layer.4.attention.self.value.bias\n",
      "75 bert.encoder.layer.4.attention.output.dense.weight\n",
      "76 bert.encoder.layer.4.attention.output.dense.bias\n",
      "77 bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "78 bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "79 bert.encoder.layer.4.intermediate.dense.weight\n",
      "80 bert.encoder.layer.4.intermediate.dense.bias\n",
      "81 bert.encoder.layer.4.output.dense.weight\n",
      "82 bert.encoder.layer.4.output.dense.bias\n",
      "83 bert.encoder.layer.4.output.LayerNorm.weight\n",
      "84 bert.encoder.layer.4.output.LayerNorm.bias\n",
      "85 bert.encoder.layer.5.attention.self.query.weight\n",
      "86 bert.encoder.layer.5.attention.self.query.bias\n",
      "87 bert.encoder.layer.5.attention.self.key.weight\n",
      "88 bert.encoder.layer.5.attention.self.key.bias\n",
      "89 bert.encoder.layer.5.attention.self.value.weight\n",
      "90 bert.encoder.layer.5.attention.self.value.bias\n",
      "91 bert.encoder.layer.5.attention.output.dense.weight\n",
      "92 bert.encoder.layer.5.attention.output.dense.bias\n",
      "93 bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "94 bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "95 bert.encoder.layer.5.intermediate.dense.weight\n",
      "96 bert.encoder.layer.5.intermediate.dense.bias\n",
      "97 bert.encoder.layer.5.output.dense.weight\n",
      "98 bert.encoder.layer.5.output.dense.bias\n",
      "99 bert.encoder.layer.5.output.LayerNorm.weight\n",
      "100 bert.encoder.layer.5.output.LayerNorm.bias\n",
      "101 bert.encoder.layer.6.attention.self.query.weight\n",
      "102 bert.encoder.layer.6.attention.self.query.bias\n",
      "103 bert.encoder.layer.6.attention.self.key.weight\n",
      "104 bert.encoder.layer.6.attention.self.key.bias\n",
      "105 bert.encoder.layer.6.attention.self.value.weight\n",
      "106 bert.encoder.layer.6.attention.self.value.bias\n",
      "107 bert.encoder.layer.6.attention.output.dense.weight\n",
      "108 bert.encoder.layer.6.attention.output.dense.bias\n",
      "109 bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "110 bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "111 bert.encoder.layer.6.intermediate.dense.weight\n",
      "112 bert.encoder.layer.6.intermediate.dense.bias\n",
      "113 bert.encoder.layer.6.output.dense.weight\n",
      "114 bert.encoder.layer.6.output.dense.bias\n",
      "115 bert.encoder.layer.6.output.LayerNorm.weight\n",
      "116 bert.encoder.layer.6.output.LayerNorm.bias\n",
      "117 bert.encoder.layer.7.attention.self.query.weight\n",
      "118 bert.encoder.layer.7.attention.self.query.bias\n",
      "119 bert.encoder.layer.7.attention.self.key.weight\n",
      "120 bert.encoder.layer.7.attention.self.key.bias\n",
      "121 bert.encoder.layer.7.attention.self.value.weight\n",
      "122 bert.encoder.layer.7.attention.self.value.bias\n",
      "123 bert.encoder.layer.7.attention.output.dense.weight\n",
      "124 bert.encoder.layer.7.attention.output.dense.bias\n",
      "125 bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "126 bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "127 bert.encoder.layer.7.intermediate.dense.weight\n",
      "128 bert.encoder.layer.7.intermediate.dense.bias\n",
      "129 bert.encoder.layer.7.output.dense.weight\n",
      "130 bert.encoder.layer.7.output.dense.bias\n",
      "131 bert.encoder.layer.7.output.LayerNorm.weight\n",
      "132 bert.encoder.layer.7.output.LayerNorm.bias\n",
      "133 bert.encoder.layer.8.attention.self.query.weight\n",
      "134 bert.encoder.layer.8.attention.self.query.bias\n",
      "135 bert.encoder.layer.8.attention.self.key.weight\n",
      "136 bert.encoder.layer.8.attention.self.key.bias\n",
      "137 bert.encoder.layer.8.attention.self.value.weight\n",
      "138 bert.encoder.layer.8.attention.self.value.bias\n",
      "139 bert.encoder.layer.8.attention.output.dense.weight\n",
      "140 bert.encoder.layer.8.attention.output.dense.bias\n",
      "141 bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "142 bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "143 bert.encoder.layer.8.intermediate.dense.weight\n",
      "144 bert.encoder.layer.8.intermediate.dense.bias\n",
      "145 bert.encoder.layer.8.output.dense.weight\n",
      "146 bert.encoder.layer.8.output.dense.bias\n",
      "147 bert.encoder.layer.8.output.LayerNorm.weight\n",
      "148 bert.encoder.layer.8.output.LayerNorm.bias\n",
      "149 bert.encoder.layer.9.attention.self.query.weight\n",
      "150 bert.encoder.layer.9.attention.self.query.bias\n",
      "151 bert.encoder.layer.9.attention.self.key.weight\n",
      "152 bert.encoder.layer.9.attention.self.key.bias\n",
      "153 bert.encoder.layer.9.attention.self.value.weight\n",
      "154 bert.encoder.layer.9.attention.self.value.bias\n",
      "155 bert.encoder.layer.9.attention.output.dense.weight\n",
      "156 bert.encoder.layer.9.attention.output.dense.bias\n",
      "157 bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "158 bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "159 bert.encoder.layer.9.intermediate.dense.weight\n",
      "160 bert.encoder.layer.9.intermediate.dense.bias\n",
      "161 bert.encoder.layer.9.output.dense.weight\n",
      "162 bert.encoder.layer.9.output.dense.bias\n",
      "163 bert.encoder.layer.9.output.LayerNorm.weight\n",
      "164 bert.encoder.layer.9.output.LayerNorm.bias\n",
      "165 bert.encoder.layer.10.attention.self.query.weight\n",
      "166 bert.encoder.layer.10.attention.self.query.bias\n",
      "167 bert.encoder.layer.10.attention.self.key.weight\n",
      "168 bert.encoder.layer.10.attention.self.key.bias\n",
      "169 bert.encoder.layer.10.attention.self.value.weight\n",
      "170 bert.encoder.layer.10.attention.self.value.bias\n",
      "171 bert.encoder.layer.10.attention.output.dense.weight\n",
      "172 bert.encoder.layer.10.attention.output.dense.bias\n",
      "173 bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "174 bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "175 bert.encoder.layer.10.intermediate.dense.weight\n",
      "176 bert.encoder.layer.10.intermediate.dense.bias\n",
      "177 bert.encoder.layer.10.output.dense.weight\n",
      "178 bert.encoder.layer.10.output.dense.bias\n",
      "179 bert.encoder.layer.10.output.LayerNorm.weight\n",
      "180 bert.encoder.layer.10.output.LayerNorm.bias\n",
      "181 bert.encoder.layer.11.attention.self.query.weight\n",
      "182 bert.encoder.layer.11.attention.self.query.bias\n",
      "183 bert.encoder.layer.11.attention.self.key.weight\n",
      "184 bert.encoder.layer.11.attention.self.key.bias\n",
      "185 bert.encoder.layer.11.attention.self.value.weight\n",
      "186 bert.encoder.layer.11.attention.self.value.bias\n",
      "187 bert.encoder.layer.11.attention.output.dense.weight\n",
      "188 bert.encoder.layer.11.attention.output.dense.bias\n",
      "189 bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "190 bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "191 bert.encoder.layer.11.intermediate.dense.weight\n",
      "192 bert.encoder.layer.11.intermediate.dense.bias\n",
      "193 bert.encoder.layer.11.output.dense.weight\n",
      "194 bert.encoder.layer.11.output.dense.bias\n",
      "195 bert.encoder.layer.11.output.LayerNorm.weight\n",
      "196 bert.encoder.layer.11.output.LayerNorm.bias\n",
      "197 bert.encoder.layer.12.attention.self.query.weight\n",
      "198 bert.encoder.layer.12.attention.self.query.bias\n",
      "199 bert.encoder.layer.12.attention.self.key.weight\n",
      "200 bert.encoder.layer.12.attention.self.key.bias\n",
      "201 bert.encoder.layer.12.attention.self.value.weight\n",
      "202 bert.encoder.layer.12.attention.self.value.bias\n",
      "203 bert.encoder.layer.12.attention.output.dense.weight\n",
      "204 bert.encoder.layer.12.attention.output.dense.bias\n",
      "205 bert.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "206 bert.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "207 bert.encoder.layer.12.intermediate.dense.weight\n",
      "208 bert.encoder.layer.12.intermediate.dense.bias\n",
      "209 bert.encoder.layer.12.output.dense.weight\n",
      "210 bert.encoder.layer.12.output.dense.bias\n",
      "211 bert.encoder.layer.12.output.LayerNorm.weight\n",
      "212 bert.encoder.layer.12.output.LayerNorm.bias\n",
      "213 bert.encoder.layer.13.attention.self.query.weight\n",
      "214 bert.encoder.layer.13.attention.self.query.bias\n",
      "215 bert.encoder.layer.13.attention.self.key.weight\n",
      "216 bert.encoder.layer.13.attention.self.key.bias\n",
      "217 bert.encoder.layer.13.attention.self.value.weight\n",
      "218 bert.encoder.layer.13.attention.self.value.bias\n",
      "219 bert.encoder.layer.13.attention.output.dense.weight\n",
      "220 bert.encoder.layer.13.attention.output.dense.bias\n",
      "221 bert.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "222 bert.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "223 bert.encoder.layer.13.intermediate.dense.weight\n",
      "224 bert.encoder.layer.13.intermediate.dense.bias\n",
      "225 bert.encoder.layer.13.output.dense.weight\n",
      "226 bert.encoder.layer.13.output.dense.bias\n",
      "227 bert.encoder.layer.13.output.LayerNorm.weight\n",
      "228 bert.encoder.layer.13.output.LayerNorm.bias\n",
      "229 bert.encoder.layer.14.attention.self.query.weight\n",
      "230 bert.encoder.layer.14.attention.self.query.bias\n",
      "231 bert.encoder.layer.14.attention.self.key.weight\n",
      "232 bert.encoder.layer.14.attention.self.key.bias\n",
      "233 bert.encoder.layer.14.attention.self.value.weight\n",
      "234 bert.encoder.layer.14.attention.self.value.bias\n",
      "235 bert.encoder.layer.14.attention.output.dense.weight\n",
      "236 bert.encoder.layer.14.attention.output.dense.bias\n",
      "237 bert.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "238 bert.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "239 bert.encoder.layer.14.intermediate.dense.weight\n",
      "240 bert.encoder.layer.14.intermediate.dense.bias\n",
      "241 bert.encoder.layer.14.output.dense.weight\n",
      "242 bert.encoder.layer.14.output.dense.bias\n",
      "243 bert.encoder.layer.14.output.LayerNorm.weight\n",
      "244 bert.encoder.layer.14.output.LayerNorm.bias\n",
      "245 bert.encoder.layer.15.attention.self.query.weight\n",
      "246 bert.encoder.layer.15.attention.self.query.bias\n",
      "247 bert.encoder.layer.15.attention.self.key.weight\n",
      "248 bert.encoder.layer.15.attention.self.key.bias\n",
      "249 bert.encoder.layer.15.attention.self.value.weight\n",
      "250 bert.encoder.layer.15.attention.self.value.bias\n",
      "251 bert.encoder.layer.15.attention.output.dense.weight\n",
      "252 bert.encoder.layer.15.attention.output.dense.bias\n",
      "253 bert.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "254 bert.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "255 bert.encoder.layer.15.intermediate.dense.weight\n",
      "256 bert.encoder.layer.15.intermediate.dense.bias\n",
      "257 bert.encoder.layer.15.output.dense.weight\n",
      "258 bert.encoder.layer.15.output.dense.bias\n",
      "259 bert.encoder.layer.15.output.LayerNorm.weight\n",
      "260 bert.encoder.layer.15.output.LayerNorm.bias\n",
      "261 bert.encoder.layer.16.attention.self.query.weight\n",
      "262 bert.encoder.layer.16.attention.self.query.bias\n",
      "263 bert.encoder.layer.16.attention.self.key.weight\n",
      "264 bert.encoder.layer.16.attention.self.key.bias\n",
      "265 bert.encoder.layer.16.attention.self.value.weight\n",
      "266 bert.encoder.layer.16.attention.self.value.bias\n",
      "267 bert.encoder.layer.16.attention.output.dense.weight\n",
      "268 bert.encoder.layer.16.attention.output.dense.bias\n",
      "269 bert.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "270 bert.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "271 bert.encoder.layer.16.intermediate.dense.weight\n",
      "272 bert.encoder.layer.16.intermediate.dense.bias\n",
      "273 bert.encoder.layer.16.output.dense.weight\n",
      "274 bert.encoder.layer.16.output.dense.bias\n",
      "275 bert.encoder.layer.16.output.LayerNorm.weight\n",
      "276 bert.encoder.layer.16.output.LayerNorm.bias\n",
      "277 bert.encoder.layer.17.attention.self.query.weight\n",
      "278 bert.encoder.layer.17.attention.self.query.bias\n",
      "279 bert.encoder.layer.17.attention.self.key.weight\n",
      "280 bert.encoder.layer.17.attention.self.key.bias\n",
      "281 bert.encoder.layer.17.attention.self.value.weight\n",
      "282 bert.encoder.layer.17.attention.self.value.bias\n",
      "283 bert.encoder.layer.17.attention.output.dense.weight\n",
      "284 bert.encoder.layer.17.attention.output.dense.bias\n",
      "285 bert.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "286 bert.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "287 bert.encoder.layer.17.intermediate.dense.weight\n",
      "288 bert.encoder.layer.17.intermediate.dense.bias\n",
      "289 bert.encoder.layer.17.output.dense.weight\n",
      "290 bert.encoder.layer.17.output.dense.bias\n",
      "291 bert.encoder.layer.17.output.LayerNorm.weight\n",
      "292 bert.encoder.layer.17.output.LayerNorm.bias\n",
      "293 bert.encoder.layer.18.attention.self.query.weight\n",
      "294 bert.encoder.layer.18.attention.self.query.bias\n",
      "295 bert.encoder.layer.18.attention.self.key.weight\n",
      "296 bert.encoder.layer.18.attention.self.key.bias\n",
      "297 bert.encoder.layer.18.attention.self.value.weight\n",
      "298 bert.encoder.layer.18.attention.self.value.bias\n",
      "299 bert.encoder.layer.18.attention.output.dense.weight\n",
      "300 bert.encoder.layer.18.attention.output.dense.bias\n",
      "301 bert.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "302 bert.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "303 bert.encoder.layer.18.intermediate.dense.weight\n",
      "304 bert.encoder.layer.18.intermediate.dense.bias\n",
      "305 bert.encoder.layer.18.output.dense.weight\n",
      "306 bert.encoder.layer.18.output.dense.bias\n",
      "307 bert.encoder.layer.18.output.LayerNorm.weight\n",
      "308 bert.encoder.layer.18.output.LayerNorm.bias\n",
      "309 bert.encoder.layer.19.attention.self.query.weight\n",
      "310 bert.encoder.layer.19.attention.self.query.bias\n",
      "311 bert.encoder.layer.19.attention.self.key.weight\n",
      "312 bert.encoder.layer.19.attention.self.key.bias\n",
      "313 bert.encoder.layer.19.attention.self.value.weight\n",
      "314 bert.encoder.layer.19.attention.self.value.bias\n",
      "315 bert.encoder.layer.19.attention.output.dense.weight\n",
      "316 bert.encoder.layer.19.attention.output.dense.bias\n",
      "317 bert.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "318 bert.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "319 bert.encoder.layer.19.intermediate.dense.weight\n",
      "320 bert.encoder.layer.19.intermediate.dense.bias\n",
      "321 bert.encoder.layer.19.output.dense.weight\n",
      "322 bert.encoder.layer.19.output.dense.bias\n",
      "323 bert.encoder.layer.19.output.LayerNorm.weight\n",
      "324 bert.encoder.layer.19.output.LayerNorm.bias\n",
      "325 bert.encoder.layer.20.attention.self.query.weight\n",
      "326 bert.encoder.layer.20.attention.self.query.bias\n",
      "327 bert.encoder.layer.20.attention.self.key.weight\n",
      "328 bert.encoder.layer.20.attention.self.key.bias\n",
      "329 bert.encoder.layer.20.attention.self.value.weight\n",
      "330 bert.encoder.layer.20.attention.self.value.bias\n",
      "331 bert.encoder.layer.20.attention.output.dense.weight\n",
      "332 bert.encoder.layer.20.attention.output.dense.bias\n",
      "333 bert.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "334 bert.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "335 bert.encoder.layer.20.intermediate.dense.weight\n",
      "336 bert.encoder.layer.20.intermediate.dense.bias\n",
      "337 bert.encoder.layer.20.output.dense.weight\n",
      "338 bert.encoder.layer.20.output.dense.bias\n",
      "339 bert.encoder.layer.20.output.LayerNorm.weight\n",
      "340 bert.encoder.layer.20.output.LayerNorm.bias\n",
      "341 bert.encoder.layer.21.attention.self.query.weight\n",
      "342 bert.encoder.layer.21.attention.self.query.bias\n",
      "343 bert.encoder.layer.21.attention.self.key.weight\n",
      "344 bert.encoder.layer.21.attention.self.key.bias\n",
      "345 bert.encoder.layer.21.attention.self.value.weight\n",
      "346 bert.encoder.layer.21.attention.self.value.bias\n",
      "347 bert.encoder.layer.21.attention.output.dense.weight\n",
      "348 bert.encoder.layer.21.attention.output.dense.bias\n",
      "349 bert.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "350 bert.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "351 bert.encoder.layer.21.intermediate.dense.weight\n",
      "352 bert.encoder.layer.21.intermediate.dense.bias\n",
      "353 bert.encoder.layer.21.output.dense.weight\n",
      "354 bert.encoder.layer.21.output.dense.bias\n",
      "355 bert.encoder.layer.21.output.LayerNorm.weight\n",
      "356 bert.encoder.layer.21.output.LayerNorm.bias\n",
      "357 bert.encoder.layer.22.attention.self.query.weight\n",
      "358 bert.encoder.layer.22.attention.self.query.bias\n",
      "359 bert.encoder.layer.22.attention.self.key.weight\n",
      "360 bert.encoder.layer.22.attention.self.key.bias\n",
      "361 bert.encoder.layer.22.attention.self.value.weight\n",
      "362 bert.encoder.layer.22.attention.self.value.bias\n",
      "363 bert.encoder.layer.22.attention.output.dense.weight\n",
      "364 bert.encoder.layer.22.attention.output.dense.bias\n",
      "365 bert.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "366 bert.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "367 bert.encoder.layer.22.intermediate.dense.weight\n",
      "368 bert.encoder.layer.22.intermediate.dense.bias\n",
      "369 bert.encoder.layer.22.output.dense.weight\n",
      "370 bert.encoder.layer.22.output.dense.bias\n",
      "371 bert.encoder.layer.22.output.LayerNorm.weight\n",
      "372 bert.encoder.layer.22.output.LayerNorm.bias\n",
      "373 bert.encoder.layer.23.attention.self.query.weight\n",
      "374 bert.encoder.layer.23.attention.self.query.bias\n",
      "375 bert.encoder.layer.23.attention.self.key.weight\n",
      "376 bert.encoder.layer.23.attention.self.key.bias\n",
      "377 bert.encoder.layer.23.attention.self.value.weight\n",
      "378 bert.encoder.layer.23.attention.self.value.bias\n",
      "379 bert.encoder.layer.23.attention.output.dense.weight\n",
      "380 bert.encoder.layer.23.attention.output.dense.bias\n",
      "381 bert.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "382 bert.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "383 bert.encoder.layer.23.intermediate.dense.weight\n",
      "384 bert.encoder.layer.23.intermediate.dense.bias\n",
      "385 bert.encoder.layer.23.output.dense.weight\n",
      "386 bert.encoder.layer.23.output.dense.bias\n",
      "387 bert.encoder.layer.23.output.LayerNorm.weight\n",
      "388 bert.encoder.layer.23.output.LayerNorm.bias\n",
      "389 bert.pooler.dense.weight\n",
      "390 bert.pooler.dense.bias\n",
      "391 linear.weight\n",
      "392 linear.bias\n",
      "393 linear1.weight\n",
      "394 linear1.bias\n",
      "395 linear2.weight\n",
      "396 linear2.bias\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "print(\"training on\", device)\n",
    "model = SBERT_base_Model(\"klue/roberta-large\")\n",
    "for i, (name,param) in enumerate(model.named_parameters()):\n",
    "    param.requires_grad = False\n",
    "    if i == 392:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SBERT_with_KLUE_BERT()\n",
    "\n",
    "epochs = 1\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(params=model.parameters(), lr=.2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight torch.Size([32000, 768])\n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.pooler.dense.weight torch.Size([768, 768])\n",
      "bert.pooler.dense.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m      6\u001b[0m     s1, s2, label \u001b[39m=\u001b[39m data\n\u001b[1;32m----> 7\u001b[0m     logits \u001b[39m=\u001b[39m model(s1, s2)\n\u001b[0;32m      8\u001b[0m     loss \u001b[39m=\u001b[39m criterion(logits, label)\n\u001b[0;32m      9\u001b[0m     train_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mdetach())\n",
      "File \u001b[1;32mc:\\Users\\rando\\Anaconda3\\envs\\boost\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rando\\Desktop\\NAVER_BOOSTCAMP\\6-7주차\\level1-nlp13\\models\\sbert_klue_bert_base.py:13\u001b[0m, in \u001b[0;36mSBERT_with_KLUE_BERT.forward\u001b[1;34m(self, src_ids, tgt_ids)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src_ids, tgt_ids):\n\u001b[1;32m---> 13\u001b[0m     u \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(src_ids)[\u001b[39m'\u001b[39m\u001b[39mpooler_output\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m     v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(tgt_ids)[\u001b[39m'\u001b[39m\u001b[39mpooler_output\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimilarity(u, v)\n",
      "File \u001b[1;32mc:\\Users\\rando\\Anaconda3\\envs\\boost\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rando\\Anaconda3\\envs\\boost\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rando\\Anaconda3\\envs\\boost\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rando\\Anaconda3\\envs\\boost\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:611\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    602\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    603\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    604\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 611\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    612\u001b[0m         hidden_states,\n\u001b[0;32m    613\u001b[0m         attention_mask,\n\u001b[0;32m    614\u001b[0m         layer_head_mask,\n\u001b[0;32m    615\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    616\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    617\u001b[0m         past_key_value,\n\u001b[0;32m    618\u001b[0m         output_attentions,\n\u001b[0;32m    619\u001b[0m     )\n\u001b[0;32m    621\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\rando\\Anaconda3\\envs\\boost\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rando\\Anaconda3\\envs\\boost\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    536\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    537\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 539\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    541\u001b[0m )\n\u001b[0;32m    542\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    544\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rando\\Anaconda3\\envs\\boost\\lib\\site-packages\\transformers\\pytorch_utils.py:247\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 247\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\rando\\Anaconda3\\envs\\boost\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m--> 551\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[0;32m    552\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\rando\\Anaconda3\\envs\\boost\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rando\\Anaconda3\\envs\\boost\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:451\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 451\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[0;32m    452\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\rando\\Anaconda3\\envs\\boost\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rando\\Anaconda3\\envs\\boost\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in train_loader:\n",
    "        s1, s2, label = data\n",
    "        logits = model(s1, s2)\n",
    "        loss = criterion(logits, label)\n",
    "        train_loss.append(loss.detach())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            s1, s2, label = data\n",
    "            logits = model(s1, s2)\n",
    "            loss = criterion(logits, label)\n",
    "            val_loss += loss\n",
    "    valid_loss.append(val_loss.detach()/i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(21.4045),\n",
       " tensor(5.1395),\n",
       " tensor(5.2421),\n",
       " tensor(7.4910),\n",
       " tensor(7.2007),\n",
       " tensor(7.2855),\n",
       " tensor(4.5699),\n",
       " tensor(3.8467)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
