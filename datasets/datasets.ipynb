{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class KorSTSDatasets(Dataset):\n",
    "    def __init__(self, dir_x, dir_y):\n",
    "        self.x = np.load(dir_x, allow_pickle=True)\n",
    "        self.y = np.load(dir_y, allow_pickle=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence1, sentence2 = self.x[idx]\n",
    "        data = torch.IntTensor(sentence1), torch.IntTensor(sentence2)\n",
    "        label = int(float(self.y[idx]))\n",
    "        return data, label\n",
    "\n",
    "dataset = KorSTSDatasets(\"../KorSTS/train_x.npy\", \"../KorSTS/train_y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((tensor([    2,  7046,  2116, 31389, 19521,  1513,  2062,    18,     3],\n",
      "       dtype=torch.int32), tensor([    2,  7046,  2116, 31389, 19521,  1513,  2062,    18,     3],\n",
      "       dtype=torch.int32)), 5)\n"
     ]
    }
   ],
   "source": [
    "for data in dataset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 19])\n",
      "torch.Size([64, 19])\n",
      "tensor([1.2000, 4.6000, 4.6000, 0.0000, 2.6000, 3.8000, 3.6000, 3.8000, 3.6000,\n",
      "        3.6000, 3.6000, 1.6000, 4.0000, 0.0000, 4.2000, 4.6000, 2.2500, 0.4000,\n",
      "        0.4000, 4.4000, 3.8000, 2.0000, 3.6000, 2.2000, 5.0000, 0.2000, 3.6000,\n",
      "        1.8000, 0.8000, 4.0000, 0.0000, 2.6000, 1.6000, 5.0000, 4.0000, 4.4000,\n",
      "        3.6000, 4.0000, 2.4000, 2.6000, 4.0000, 0.2000, 3.2000, 3.0000, 4.2000,\n",
      "        4.2500, 4.2000, 3.8000, 0.8000, 3.0000, 3.8000, 0.0000, 0.2000, 4.6000,\n",
      "        3.0000, 4.4000, 5.0000, 3.6000, 0.0000, 4.0000, 1.4000, 3.2000, 4.2000,\n",
      "        3.4000])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def collate_fn_(batch):\n",
    "    # batch = list([((s1, s2), label), ((s1, s2), label), ...])\n",
    "    s1_batches = []\n",
    "    s2_batches = []\n",
    "    labels = []\n",
    "    for b in batch:\n",
    "        data, label = b\n",
    "        s1, s2 = data\n",
    "        s1_batches.append(s1)\n",
    "        s2_batches.append(s2)\n",
    "        labels.append(label)\n",
    "        \n",
    "    s1_batch = pad_sequence(s1_batches, batch_first=True, padding_value=0)\n",
    "    s2_batch = pad_sequence(s2_batches, batch_first=True, padding_value=0)\n",
    "    return s1_batch, s2_batch, torch.Tensor(labels)\n",
    "\n",
    "def bucketed_batch_indices(\n",
    "    sentence_length: List[Tuple[int, int]],\n",
    "    batch_size: int,\n",
    "    max_pad_len: int\n",
    "):\n",
    "    batch_indices_list = []\n",
    "    bucket = defaultdict(list)\n",
    "    for idx, length in enumerate(sentence_length):\n",
    "        s1_len, s2_len = length\n",
    "        x = s1_len//max_pad_len\n",
    "        y = s2_len//max_pad_len\n",
    "        bucket[(x, y)].append(idx)\n",
    "        if len(bucket[(x, y)]) == 64:\n",
    "            batch_indices_list.append(bucket[(x, y)])\n",
    "            bucket[(x, y)] = []\n",
    "    for key in bucket.keys():\n",
    "        batch_indices_list.append(bucket[key])\n",
    "\n",
    "    random.shuffle(batch_indices_list)\n",
    "\n",
    "    return batch_indices_list\n",
    "\n",
    "sentence_length = []\n",
    "for s1, s2 in dataset.x: # [(s1, s2), (s1, s2), ...]\n",
    "    sentence_length.append((len(s1), len(s2)))\n",
    "\n",
    "sampler = bucketed_batch_indices(sentence_length, batch_size=64, max_pad_len=10)\n",
    "train_dataloader = DataLoader(dataset, collate_fn=collate_fn_, batch_sampler=sampler)\n",
    "\n",
    "for data in train_dataloader:\n",
    "    s1, s2, label = data\n",
    "    print(s1.shape)\n",
    "    print(s2.shape)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550 1100\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "\n",
    "\n",
    "class KorSTSDatasets(Dataset):\n",
    "    def __init__(self, dir_x: str, dir_y: str, bi_directional_training: bool=False):\n",
    "        self.x = np.load(dir_x, allow_pickle=True)\n",
    "        self.y = np.load(dir_y, allow_pickle=True)\n",
    "\n",
    "        if bi_directional_training:\n",
    "            bi_x = list(self.x)\n",
    "            for (s1, s2) in self.x:\n",
    "                bi_x.append((s2, s1))\n",
    "            self.x = bi_x\n",
    "            self.y = np.concatenate((self.y, self.y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence1, sentence2 = self.x[idx]\n",
    "        data = torch.IntTensor(sentence1), torch.IntTensor(sentence2)\n",
    "        # cosine similarity의 범위 [-1. ~ 1.] 사이 값으로 정규화 필요.\n",
    "        # label = float(self.y[idx]) * 0.4 - 1\n",
    "        \n",
    "        label = float(self.y[idx])\n",
    "        return data, label\n",
    "   \n",
    "datasets = KorSTSDatasets(\"../NLP_dataset/klue/roberta-large/valid_x.npy\", \"../NLP_dataset/klue/roberta-large/valid_y.npy\", False)\n",
    "bi_datasets = KorSTSDatasets(\"../NLP_dataset/klue/roberta-large/valid_x.npy\", \"../NLP_dataset/klue/roberta-large/valid_y.npy\", True)\n",
    "\n",
    "print(len(datasets), len(bi_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([list([0, 8765, 2073, 2019, 3306, 1668, 3325, 2517, 3363, 2088, 711, 12190, 2]),\n",
       "        list([0, 8765, 2073, 12961, 2154, 30524, 636, 2057, 2190, 2119, 19824, 2170, 2259, 8530, 2137, 2391, 2197, 4211, 2])],\n",
       "       dtype=object),\n",
       " ([0,\n",
       "   8765,\n",
       "   2073,\n",
       "   12961,\n",
       "   2154,\n",
       "   30524,\n",
       "   636,\n",
       "   2057,\n",
       "   2190,\n",
       "   2119,\n",
       "   19824,\n",
       "   2170,\n",
       "   2259,\n",
       "   8530,\n",
       "   2137,\n",
       "   2391,\n",
       "   2197,\n",
       "   4211,\n",
       "   2],\n",
       "  [0, 8765, 2073, 2019, 3306, 1668, 3325, 2517, 3363, 2088, 711, 12190, 2]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "datasets.x[0+i], bi_datasets.x[550+i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
